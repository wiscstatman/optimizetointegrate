\documentclass[12pt]{article}
\usepackage{amsmath,amssymb,amsthm,amsfonts,bm,bbm}
\usepackage{graphicx,fancyhdr,epstopdf}
\usepackage{enumerate,mathrsfs,multirow}
\usepackage{natbib}

\DeclareMathOperator*{\argmax}{arg\,max} %argmax
\DeclareMathOperator*{\argmin}{arg\,min} %argmin

\newcommand{\EX}{\mathbb{E}} % expected value
\newcommand{\bnw}{\widehat{\bm{\beta}}_n^w} % weighted lasso estimator
\newcommand{\bhat}{\widehat{\bm{\beta}}_n} % strongly consistent estimator
\newcommand{\bLAS}{\widehat{\bm{\beta}}_n^{\text{LAS}}} % lasso estimator
\newcommand{\bLS}{\widehat{\bm{\beta}}_n^{\text{OLS}}} % LSE 
\newcommand{\be}{\bm{\beta}} % beta vector
\newcommand{\ep}{\bm{\epsilon}} % epsilon vector
\newcommand{\eLS}{\bm{e}_n^{\text{OLS}}} % LSE residual vector
\newcommand{\sumin}{\sum_{i=1}^n} % sum from i = 1 to n
\newcommand{\dn}{\frac{1}{n}} % 1/n
\newcommand{\dqn}{\frac{1}{\sqrt{n}}} % 1/sqrt{n}
\newcommand{\CONV}[1]{\stackrel{\text{#1}}{\longrightarrow}} % convergence mode
\newcommand{\overbar}[1]{\mkern 2mu\overline{\mkern-2mu#1\mkern-2mu}\mkern 2mu}
\newcommand{\x}{\bm{x}_i} % i-th row of design matrix X
\newcommand{\bu}{\bm{u}} % vector u

\newcommand{\cnwa}{C_{n(11)}^w}
\newcommand{\cnwb}{C_{n(12)}^w}
\newcommand{\cnwc}{C_{n(21)}^w}
\newcommand{\cnwd}{C_{n(22)}^w}
\newcommand{\znwa}{\bm{Z}_{n(1)}^w}
\newcommand{\znwb}{\bm{Z}_{n(2)}^w}
\newcommand{\huna}{\widehat{\bu}_{n(1)}}
\newcommand{\hunb}{\widehat{\bu}_{n(2)}}

\newcommand{\cnwas}{C_{n(11*)}^w}
\newcommand{\cnwbs}{C_{n(12*)}^w}
\newcommand{\cnwcs}{C_{n(21*)}^w}
\newcommand{\cnwds}{C_{n(22*)}^w}
\newcommand{\znwas}{\bm{Z}_{n(1*)}^w}
\newcommand{\znwbs}{\bm{Z}_{n(2*)}^w}
\newcommand{\hunas}{\widehat{\bu}_{n(1*)}}
\newcommand{\hunbs}{\widehat{\bu}_{n(2*)}}
\newcommand{\cnwsi}{\left(\bm{c}_{n,i}^{w*}\right)} %i-th row of C_n^{w*}

\newtheorem{thm}{Theorem}[section]
\newtheorem{corollary}{Corollary}[thm]
\newtheorem{lem}{Lemma}[section] 

\begin{document}

\title{Weighted Lasso Bootstrap}
\author{
	Tun Lee Ng
	\and
	Michael A. Newton
}\date{\today}
\maketitle

\section{Introduction}

Consider the following linear regression model 
	\begin{align} \label{linear.reg.1}
	Y_i = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_p x_{ip} + \epsilon_i,  
	\end{align}
\noindent for $i = 1, \ldots, n$, and $\{\epsilon_i\}$ are independent and identically distributed (iid) random variables with mean 0 and finite variance $\sigma^2$. We assume that $p$ is fixed. Without loss of generality, the covariates are centered to have mean 0, so that $\hat{\beta}_0 = \bar{Y}$, and $Y_i$ in \eqref{linear.reg.1} can be replaced by $Y_i - \bar{Y}$. Again, without loss of generality, we assume that $\bar{Y} = 0$. Then, \eqref{linear.reg.1} can be expressed as the following         
	\begin{align} \label{linear.reg.2}
	Y_i = \x' \be + \epsilon_i,  
	\end{align}
\noindent where $Y_i$ is the centered response, $\x' = (x_{i1}, \ldots, x_{ip})$ is the $p \times 1$  centered covariate vector and $\be = (\beta_1, \ldots, \beta_p)'$ are the regression parameters. \\

Throughout this paper, we assume that our predictor matrix $X$ satisfies the following conditions:
	\begin{align} \label{cond_max_X_i} 
	\underset{1 \leq i \leq n}{\text{max}} \| \x  \|_2^2 = \mathcal{O} (1) 
	\quad \text{as } n \to \infty,
	\end{align}
and there exists a non-singular matrix $C$ such that
	\begin{align} \label{cond_X'X}
	\dn X'X = \dn \sumin \x \x' \to C 
	\quad \text{as } n \to \infty.
	\end{align}

Let $\be_0$ be the true values of the regression parameters $\be$. The model is assumed to be sparse, ie. some of the elements of $\be_0$ are exactly zero corresponding to predictors that are irrelevant to the response. \\  

The Lasso estimator is defined to be the minimizer of the $l_1$-penalized least square objective function,
	\begin{align} \label{lasso.obj}   
	\widehat{\be}_n 
	:= \argmin_{\be} 
		\sumin ( y_i - \x' \be )^2 
		+ \lambda_n \sum_{j=1}^p |\beta_j|
	\end{align}
\noindent for a given penalty or regularization parameter $\lambda_n$. The Lasso estimator was first introduced by \citet{Lasso}. \citet{Knight&Fu} obtained the asymptotic distribution of the Lasso estimator and showed that the Lasso is weakly consistent under some mild regularity condition. \citet{Chatterjee&Lahiri} studied strong consistency of the Lasso estimator under a slightly more stringent regularity condition. \\

Following the idea by \citet{Newton&Raftery}, for a given set of responses $\bm{y} = (y_1, \ldots, y_n)'$, we define the weighted Lasso estimator as follows:
	\begin{align} \label{weighted.lasso.obj}   
	\bnw := \argmin_{\be} 
			\sumin \widetilde{w}_i ( y_i - \x' \be )^2 
			+ \lambda_n \widetilde{w}_{n+1} \sum_{j=1}^p |\beta_j|.
	\end{align}
\noindent Here, $\widetilde{\bm{w}} = ( \widetilde{w}_1, \ldots, \widetilde{w}_{n+1} )$ are random weights drawn from
	\begin{align*}
	\left(
		\dfrac{W_1}{ \sum_{i=1}^{n+1} W_i}
		, \ldots, 
		\dfrac{ W_{n+1} }{ \sum_{i=1}^{n+1} W_i}
	\right)
	= 
	\left(
		\dfrac{W_1}{ (n+1) \overbar{W} }
		, \ldots, 
		\dfrac{ W_{n+1} }{ (n+1) \overbar{W} }
	\right),
	\end{align*} 
\noindent where $W_1, \ldots, W_n$ $\stackrel{iid}{\sim}$ exp(1) and $W_{n+1} = 1$ a.s. Note that the random weights $\widetilde{\bm{w}}$ are generated independently of the data $\bm{y}$, and are similar in structure to a Dirichlet weight vector as expounded by \citet{Newton&Raftery}. \\

Hence, for a given set of data $\bm{y}$, \eqref{weighted.lasso.obj} can be expressed as
	\begin{align} \label{weighted.lasso.obj2}   
	\bnw = \argmin_{\be}
			\dfrac{1}{\overbar{W}}
			\left\{
				\dn \sumin W_i ( y_i - \x' \be )^2 
				+ \dfrac{\lambda_n}{n} \sum_{j=1}^p |\beta_j|
			\right\}. 
	\end{align}
For any given set of data, the sampling distribution of $\left\{ \widehat{\be}^w_{n,k} \right\}^K_{k=1}$ is induced by the randomly drawn weights $\left\{ \widetilde{\bm{w}}_k \right\}^K_{k=1}$.         	  	  


\section{Asymptotics for WLB}

Need to define the proper probability space \citep{newton1991}... \\
Need to define ``\textit{convergence in conditional probability}" (denoted with $\CONV{c.p.}$)... \\
Need to define ``\textit{convergence in conditional distribution}" (denoted with $\CONV{c.d.}$)... \\
Need restrictions on the topology of parameter space ($\be \in \Theta$ for open, convex subset $\Theta$ of $\mathcal{R}^p$)...\\

In this section, we provide theoretical results about the asymptotic properties of the Weighted Lasso Bootstrap. We prove its conditional consistency property and its asymptotic conditional distributions under certain conditions. We will also show that it has model selection consistency by conditioning on data. First, we introduce some notations. \\

\textbf{Notations:} The symbol "$\stackrel{d}{\approx}$" denotes "approximately distributed". For any matrix $A$, $\gamma_{\text{min}} (A)$ refers to the smallest eigenvalue of $A$, whereas $\gamma_{\text{max}} (A)$ refers to the largest eigenvalue of $A$. \\

Next, without loss of generality, suppose that out of the $p$ predictors, $q$ of them are relevant, and the remaining $p-q$ of them irrelevant. Obviously, we are only interested in the non-trivial case where $1 \leq q \leq p$. Then the columns of $X$ can be partitioned into 
	\[
		X = 
		\begin{bmatrix}
			X_{(1)} & X_{(2)}
		\end{bmatrix}
	\]
which corresponds to those relevant and non-relevant predictors respectively. Similarly, $\be_0$ can be partitioned into 
	\[
		\be_0 = 
		\begin{bmatrix}
			\be_{0(1)} \\
			\be_{0(2)}
		\end{bmatrix}
		,
	\]
where $\be_{0(1)}$ refers to the $q \times 1$ vector of non-zero true regression parameters corresponding to $X_{(1)}$, and $\be_{0(2)}$ is a $(p-q) \times 1$ zero vector that corresponds to $X_{(2)}$. In addition, define 
	\[
		\begin{bmatrix}
			C_{n(11)} & C_{n(12)} \\
			C_{n(21)} & C_{n(22)} 
		\end{bmatrix}
		:= \dn X'X = \dn
		\begin{bmatrix}
			X'_{(1)} X_{(1)} & X'_{(1)} X_{(2)} \\
			X'_{(2)} X_{(1)} & X'_{(2)} X_{(2)}
		\end{bmatrix}
	\]
which, by our assumption \eqref{cond_X'X}, converges to 
	\[
		C = 
		\begin{bmatrix}
			C_{11} & C_{12}\\
			C_{21} & C_{22}
		\end{bmatrix}
		.
	\]	
Furthermore, let $D_n = diag(W_1, \ldots, W_n)$, and define 
	\[
		\begin{bmatrix}
			\cnwa & \cnwb \\
			\cnwc & \cnwd 
		\end{bmatrix}
		:= \dn X' D_n X = \dn
		\begin{bmatrix}
			X'_{(1)} D_n X_{(1)} & X'_{(1)} D_n X_{(2)} \\
			X'_{(2)} D_n X_{(1)} & X'_{(2)} D_n X_{(2)}
		\end{bmatrix}
		.
	\]
Finally, an estimator $\bhat$ is said to be equal in sign to the true parameter $\be_0$, if 
	$$
	\text{sgn}(\bhat) = \text{sgn}(\be_0),
	$$
and is denoted as
	$$
	\bhat \stackrel{s}{=} \be_0.
	$$	
An important lemma, which underlies all our asymptotic results, is given below.

\begin{lem} \label{lem_slutsky}
	\textbf{(Conditional Slutsky's)} Consider two sequences $\{V_n\}$ and $\{U_n\}$ and two other random variables $V$ and $U$, all defined on the same product space $(\Omega, \mathcal{F})$. If
	$$
	V_n | \text{data} \CONV{c.p.} V 
	\qquad \text{and} \qquad
	U_n | \text{data} \CONV{c.d.} U,
	$$
	then
	$$
	(V_n U_n) | \text{data} \CONV{c.d.} V U 
	\qquad \text{and} \qquad 
	(V_n + U_n) | \text{data} \CONV{c.d.} V + U.
	$$
\end{lem} 

\begin{proof}
	For each fixed infinite sequence of data, the results follow from properties of convergence in distribution due to Slutsky's theorem.
\end{proof}

\begin{thm} \label{consistency}
	Consider the linear regression model in \eqref{linear.reg.2}, with the predictor matrix $X$ satisfying \eqref{cond_max_X_i} and \eqref{cond_X'X}. 
	\begin{itemize}
		\item [ (a) ] \textbf{(Conditional Consistency)} If $\dfrac{\lambda_n}{n} \to 0$, then
			  $$
			  \bnw \big| \text{data} \CONV{c.p.} \be_0.
			  $$
		\item [ (b) ] 	If $\dfrac{\lambda_n}{n} \to \lambda_0 \in (0,\infty)$, then 
			  $$
			  \left( \bnw - \be_0 \right) \bigg| \text{data} \CONV{c.p.} \argmin g,
			  $$
			  where
			  \begin{align*} 
					g(\bu) = \bu' C \bu 
					+ \lambda_0 \| \be_0 + \bu \|_1 .
			  \end{align*}  
	\end{itemize} 
\end{thm}

\begin{thm} \label{asympdistn}
	\textbf{(Asymptotic Conditional Distribution)} Consider the linear regression model in \eqref{linear.reg.2}, with the predictor matrix $X$ satisfying \eqref{cond_max_X_i} and \eqref{cond_X'X}. In addition, we assume that 
	\begin{align} \label{cond_epsilon4}
	\EX \left( \epsilon_i^4 \right) < \infty 
	\quad \forall \quad i.
	\end{align}
	Let $\bLS$ be the ordinary least squares (OLS) estimator for $\be$ in the linear model \eqref{linear.reg.2} with the aforementioned assumptions.
	\begin{itemize}
		\item [ (a) ] If $\dfrac{\lambda_n}{\sqrt{n}} \to 0$, then
				$$
				\sqrt{n} \left( 
				\bnw - \bLS 
				\right) 
				\bigg| \text{data} 
				\CONV{c.d.} 
				N \left( \bm{0}, \sigma^2 C^{-1} \right).
				$$	 
		\item [ (b) ] 	If $\dfrac{\lambda_n}{\sqrt{n}} \to \lambda_0 \in (0,\infty)$, then
				$$
				\sqrt{n} \left( 
				\bnw - \bLS 
				\right) 
				\bigg| \text{data} 
				\stackrel{d}{\approx} 
				\argmin \left(
							V_n^* \big| \text{data}
						\right),
				$$
			where     
				$$
				V_n^*(\bu) = -2 \bu' \Psi + \bu' C \bu 
				+ \lambda_0 \sum_{j=1}^p 
				\left[
					u_j \, \text{sgn}(\widehat{\beta}_{n,j}) 
						\mathbbm{1}_{ \{ \widehat{\beta}_{n,j} \neq 0 \}}
					+ | u_j | \mathbbm{1}_{ \{ \widehat{\beta}_{n,j} = 0 \}} 
				\right],
				$$
			for $\Psi \sim N \left( \bm{0} , \sigma^2 C \right)$.
		\item [ (c) ] Suppose we further assume that the non-invertible matrix $C$ satisfies the following assumptions:
			$$
				C_{12} = \bm{0}
				\qquad \text{and} \qquad 
				\dfrac{ \gamma_{\text{min}} ( C ) }
					  { \gamma_{\text{max}} ( C_{11} ) } 
				> \dfrac{ q-1 }{ q }. 
			$$
			Let $\bLAS$ be the corresponding strongly consistent Lasso estimator with its own penalty term $\lambda_n^*$ that satisfies
			$$
				\lambda_n^* 
				= \mathcal{O} ( n ^ {\delta_2} )
				\qquad \text{for some} \quad
				\frac{1}{2} < \delta_2 < 1.	
			$$  
			If $\dfrac{\lambda_n}{\sqrt{n}} \to \lambda_0 \in (0,\infty)$, then
				$$
				\sqrt{n} \left( 
				\bnw - \bLAS 
				\right) 
				\bigg| \text{data} 
				\stackrel{d}{\approx} 
				\argmin \left(
							V_n^{**} \big| \text{data}
						\right),
				$$
			where   
				$$
				V_n^{**}(\bu) = -2 \bu' \Psi_n + \bu' C \bu 
				+ \lambda_0 \sum_{j=1}^p 
				\left[
				u_j \, \text{sgn}(\beta_{0,j}) \mathbbm{1}_{ \{ \beta_{0,j} \neq 0 \}}
				+ | u_j | \mathbbm{1}_{ \{ \beta_{0,j} = 0 \}} 
				\right],
				$$
			for 
			$$
			\Psi_n \sim N \left( \dqn X' \bm{e}_n^{\text{LAS}} , \sigma^2 C \right),
			$$
			and $\bm{e}_n^{\text{LAS}} = Y - X \bLAS$ .   
	\end{itemize}  
\end{thm} 

\begin{thm} \label{model.select.consistent}
	\textbf{(Conditional Model Selection Consistency)} Consider the linear regression model in \eqref{linear.reg.2}, with assumptions \eqref{cond_max_X_i}, \eqref{cond_X'X} and \eqref{cond_epsilon4}. In addition, assume the \textbf{strong irrepresentable conditions} \citep{BinYu}: There exists a positive constant vector $\bm{\eta}$ such that 
	\begin{align} \label{strongirrepresent}
		\left|
			C_{n(21)} 
			\left( C_{n(11)} \right)^{-1} 
			\text{sgn} \left( \be_{0(1)} \right)
		\right| \leq
		\bm{J} - \bm{\eta},
	\end{align}
	where $\bm{J}$ is a $(p \times q)$ vector of ones and the inequality holds element wise. Then, for all $\lambda_n$ that satisfies 
		$$
		 \lambda_n = \mathcal{O} (n^c)
		 \qquad \text{for some} \quad
		 \frac{1}{2} < c < 1,
		$$
	we have
		$$
		P\left(
			\bnw (\lambda_n) \stackrel{s}{=} \be_0
			\bigg| \text{data}
		\right)	
		= 1 - o\left(e ^ {-n ^ { 2c - 1 } }\right), 
		$$ 
	and hence
		$$
		P \left(
				\text{WLB selects true model }
				\big| \text{data}
			\right)
		\geq 
		P\left( 
			\bnw (\lambda_n) \stackrel{s}{=} \be_0
			\bigg| \text{data}
		 \right)
		 \to 1 \,\,\, \text{as} \,\,\, n \to \infty.		
		$$ 
\end{thm} 

\begin{corollary}
	\citet{BinYu} established sufficient conditions for the Strong Irrepresentable condition in Theorem \ref{model.select.consistent}. These sufficient conditions are basically additional assumptions on the predictor matrix $X$. We refer readers to \citet{BinYu} for a proof of these sufficient conditions. 
\end{corollary}

\textbf{Discussion on Theorem \ref{asympdistn}:} With our assumption \eqref{cond_X'X}, $\bLS$ is a strongly consistent estimator for $\be$ in \eqref{linear.reg.2} (See, for example, \citet{LSEstrong}). In fact, $\bLS$ in parts (a) and (b) of Theorem \ref{asympdistn} can be replaced by any strongly consistent estimator $\bhat$ that satisfies
\begin{align} \label{LAS.OLS.shrink.rate}
	\sqrt{n} 
	\left(
		\bhat - \bLS
	\right)
	\bigg| \text{data}
	\CONV{c.p.} \bm{0}.
\end{align}
Any regular Lasso estimator $\bLAS$ with a penalty term $\lambda_n^* =  o( \sqrt{n} )$ could be a candidate for $\bhat$ in this case. The asterisk in $\lambda_n^*$ is to distinguish it from the penalty term $\lambda_n$ that we pick for the Weighted Lasso bootstrap. However, this rate of convergence for $\lambda_n^*$ is different from the one required in part (c) of Theorem \ref{asympdistn}. The additional assumptions on the matrix $C$ in part(c) of the theorem can be achieved by specific experimental design \citep{Chatterjee&Lahiri}. Part (c) of the theorem is to illustrate the fact that we could capitalize on the properties of Lasso estimators to remove the dependency of penalty term on sample path in part (b) of the theorem, but in the process, we introduce another term that depends on the sample path in the location parameter (mean) of $\Psi_n$. We refer readers to the Appendix section for a more detailed derivation and discussion of Theorem \ref{asympdistn}. \\

\textbf{Discussion on Theorem \ref{model.select.consistent}:} We establish that the Weighted Lasso Bootstrap exhibits conditional model selection consistency given data. Loosely-speaking, we would like to show that as $n$ gets larger, it becomes more likely for the zero elements in $\bnw$ to match those in $\be_0$. It is imperative to note that, in general, consistency in parameter estimation does not translate into model selection consistency, and vice versa. A typical example would be the OLS approach, where it produces consistent estimators but generally performs poorly in model selection. Our approach is similar to \citet{BinYu}, where we show that the Weighted Lasso Bootstrap exhibits conditional sign consistency, which then implies conditional model selection consistency.  

\section{Appendix}

Here are the proofs for the theorems in this paper.

\begin{lem} \label{lem_X'DnX}
	Assume \eqref{cond_max_X_i} and \eqref{cond_X'X}. Then, as $n \to \infty$, 
		\begin{align} \label{lemma1result}
		\dn X' D_n X
		\CONV{a.s.} C 
		\end{align}
\end{lem}

\allowdisplaybreaks
\begin{proof}
	Note that $\dn X'X$ always has a fixed $p \times p$ dimension. Coupled with assumption \eqref{cond_max_X_i}, the Strong Law of Large Numbers ensures that 
	$$
	\dn X' (D_n - I) X 
	= \dn \sumin (W_i - 1) \x \x' 
	\CONV{a.s.} \bm{0}.
	$$ 
	Assumption \eqref{cond_X'X} tells us that 
	$$
	\dn X'X
	\to C.
	$$
	Therefore, by Continuous Mapping Theorem,
	$$
	\dn X' D_n X 
	= \dn X' (D_n - I) X + \dn X'X
	\CONV{a.s.} C
	$$ 
\end{proof}

\begin{lem} \label{lem_X'DnEp}
	Assume \eqref{cond_max_X_i} and \eqref{cond_X'X}. Furthermore, let the error terms $\{\epsilon_i\}$ be iid with mean 0 and variance $\sigma^2$. Then,  
	\begin{align}
	\left(
		 \dn X'D_n \ep 
	\right) 
	\bigg| \text{data} 
	\CONV{c.p.} \bm{0}.
	\end{align}   
\end{lem}

\begin{proof}
	First, by Jensen's inequality, 
		$$
		\EX |X| = \EX (\sqrt{X^2}) 
				\leq \sqrt{\EX (X^2)} 
				\leq \EX X^2
				= \sigma^2 
				< \infty.
		$$
	Coupled with assumption \eqref{cond_max_X_i}, conditional on data,
	\begin{align} \label{constraint1} 
		\dn \sumin \|\epsilon_i \x \|_2
		\leq \underset{1 \leq i \leq n}{\text{max}} \|\x \|_2  
			\times \dn \sumin |\epsilon_i| 
		< \infty
	\end{align}
	for almost every sample path $\omega$. In addition, since $\EX(\epsilon_i) = 0$, Lemma 3.1 of \citet{Chatterjee&Lahiri} gives
		$$
		\dn \sumin \epsilon_i \x
		\CONV{a.s.} \bm{0},
		$$
	which implies that 
	\begin{align} \label{constraint2}
	\dn 
	\sumin \epsilon_i \x
	\bigg| \text{data}
	\CONV{c.p.} \bm{0}.
	\end{align}
	Furthermore, note that by conditioning on data, 
	\begin{align} \label{constraint3} 
	\dn \underset{1 \leq i \leq n}{\text{max}}  \|\epsilon_i \x \|_2 
	&= \dn \underset{1 \leq i \leq n}{\text{max}} |\epsilon_i| \|\x\|_2 \notag \\ 
	&\leq \dn \underset{1 \leq i \leq n}{\text{max}} |\epsilon_i| 
		\times \underset{1 \leq i \leq n}{\text{max}} \|\x\|_2 \notag \\
	&\CONV{c.p.} 0,
	\end{align}
	where the last line follows from the fact that 
	$$
	\EX |\epsilon_1| < \infty
	\implies 
	\dn \underset{1 \leq i \leq n}{\text{max}} |\epsilon_i| \CONV{a.s.} 0
	$$ 
	by Lemma 14 of \citet{newton1991}. \\
	
	Conditional on data, $\ep$ is fixed albeit unobservable. Hence, for every $t > 0$, by the multi-dimensional Chebyshev's inequality, 
	\begin{align*}
	&P \left( 
			\left\|  
					\dn X' D_n \ep - \bm{0} 
			\right\|_2  
			\geq t \bigg| \text{data}  
	  \right) \\
	&\leq \dfrac{1}{t^2}
		 \EX_{\bm{W}} \left\{ 
		 			  	 \left\| 
		 			  		\dn X' D_n \ep 
		 			  	 \right\|_2^2 \bigg| \text{data}
		 			  \right\} \\
	&= \dfrac{1}{t^2}
			\EX_{\bm{W}} 
			\left\{ 
				\left\| 
					\dn \sumin \epsilon_i \x W_i 
				\right\|_2^2 
				\quad \text{given data}
			\right\} \\
	&\leq \dfrac{2}{t^2}
			\left\{
				\EX_{\bm{W}} 
				\left\| 
						\dn \sumin \epsilon_i \x (W_i - 1)
				\right\|_2^2
				+
				\left\|
						\dn \sumin \epsilon_i \x 
				\right\|_2^2 
				\quad \text{given data}
			\right\} \\
	&= \dfrac{2}{t^2}
			\left\{
					\dfrac{1}{n^2} \sumin
					\left\| 
						\epsilon_i \x 
					\right\|_2^2
					+
					\left\|
						\dn \sumin \epsilon_i \x 
					\right\|_2^2 
					\quad \text{given data}
			\right\} \\
	&\leq \dfrac{2}{t^2}
			\left\{
				\dn \underset{1 \leq i \leq n}{\text{max}} 
				\| \epsilon_i \x \|_2 
				\times
				\dn \sumin \| \epsilon_i \x \|_2 
				+ 
				\left\|
					\dn \sumin \epsilon_i \x 
				\right\|_2^2   
			\quad \text{given data}
			\right\} \\
	&\to 0
	\end{align*}
	due to \eqref{constraint1}, \eqref{constraint2} and \eqref{constraint3}. Finally, by Lemma 3 of \citet{newton1991},
	$$
	\left( \dn X'D_n \ep \right) \bigg| \text{data} \CONV{c.p.} \bm{0}.
	$$ 
\end{proof}

Now we are ready to prove Theorem \ref{consistency}. 

\begin{proof} [Proof of Theorem \ref{consistency}]
	From \eqref{weighted.lasso.obj2}, conditional on data, 
	\begin{align*}
	\bnw &= \argmin_{\be}
					\dfrac{1}{\overbar{W}}
					\left\{
							\dn (Y - X \be)' D_n (Y - X \be)  
							+ \dfrac{\lambda_n}{n} \| \be \|_1
					\right\} \\
		 &= \argmin_{\be}
		 			\dfrac{1}{\overbar{W}}
		 			\left\{
		 					\dn [\ep - X (\be - \be_0)]' D_n [\ep - X (\be - \be_0)]
		 					+ \dfrac{\lambda_n}{n} \| \be_0 + \be - \be_0 \|_1  
		 			\right\}.  
	\end{align*}
	Therefore, 
	\begin{align*}
	(\bnw - \be_0) &= \argmin_{\bu}
							 \dfrac{1}{\overbar{W}}
							 \left\{
		 							\dn (\ep - X \bu)' D_n (\ep - X \bu)
									+ \dfrac{\lambda_n}{n} \| \be_0 + \bu \|_1  
							 \right\} \\
				   &= \argmin_{\bu}
							 \dfrac{1}{\overbar{W}}
							 \left\{
							 		\dn [-2 \bu' (X' D_n \ep) + \bu' (X' D_n X) \bu]
							 		+ \dfrac{\lambda_n}{n} \| \be_0 + \bu \|_1  
							 \right\}. 
	\end{align*}
	The strong law of large numbers ensures that $\overbar{W} \CONV{a.s.} 1$. Let
	$$
	g_n(\bu) 
	:= -2 \bu' \left( \dfrac{X' D_n \ep}{n} \right) 
		+ \bu' \left( \dfrac{X' D_n X}{n} \right) \bu 
		+ \dfrac{\lambda_n}{n} \| \be_0 + \bu \|_1. 
	$$ 
	By Lemma \ref{lem_X'DnX}, we have 
	$$
	\dn X' D_n X \CONV{p} C.
	$$
	By Lemma \ref{lem_X'DnEp}, we have 
	$$
	\left( \dn X' D_n \ep \right) \bigg| \text{data} \CONV{c.p.} \bm{0}.
	$$
	Hence,  if $\dfrac{\lambda_n}{n} \to \lambda_0 \in [0 , \infty)$, then by Lemma \ref{lem_slutsky}, 
	$$
	g_n(\bu) \big| \text{data}  \CONV{c.p.} g(\bu) \equiv \bu' C \bu + \lambda_0 \| \be_0 + \bu \|.
	$$
	Note that $g_n(\bu)$ is a sequence of random convex functions of $\bu$. Hence, by the Convexity Lemma \citep{Pollard1991}, for a compact set $K \subset \Theta$, where $\Theta$ is itself a convex, open subset of $\mathcal{R}^p$,
	$$
	\underset{\bu \in K \subset \Theta}{\text{sup}} \; 
	|g_n (\bu) - g (\bu)|
	\bigg| \text{data}
	\CONV{c.p.} 0. 	
	$$
	Also, note that $ \left( \bnw \big| \text{data} \right) = O_p(1)$.
	Therefore,
	\begin{align*}
	&\left( \bnw - \be_0 \right) \bigg| \text{data} \\  
	&= 	\argmin_{\bu}
		\left\{
			\dfrac{1}{\overbar{W}} g_n(\bu)
			\bigg| \text{data}  
		\right\} \\ 
	&\CONV{c.p.} \argmin_{\bu} g(\bu).
	\end{align*} 
	It follows that if $\lambda_0 = 0$, then $\argmin_{\bu} g(\bu) = \bm{0}$, i.e. $\bnw \big| \text{data} \CONV{c.p.} \be_0$.
\end{proof}

\begin{lem} \label{lem_ave_ei_xi^2} 
	Assume \eqref{cond_max_X_i} and \eqref{cond_X'X}. Let $\bhat$ be a strongly consistent estimator for $\be$ in the linear model \eqref{linear.reg.2} with assumptions \eqref{cond_max_X_i} and \eqref{cond_X'X}. Then
	$$
	\left(
		\dn \sumin e_i^2 \x \x'
	\right) \bigg| \text{data}
	\CONV{c.p.} 
	\sigma^2 C, 
	$$
	where $e_i = Y_i - \x' \bhat$.
\end{lem}

\begin{proof}
	Without loss of generality, we first consider the univariate case. 
	\begin{align*}
	e_i^2
	&= \left( Y_i - \widehat{Y_i} \right)^2 \\
	&= Y_i^2 
		+ x_i^2 \left( \widehat{\beta}_n \right)^2
		- 2 \widehat{\beta}_n x_i Y_i \\
	&= \left( x_i \beta_0 + \epsilon_i \right)^2
		+ x_i^2 \left( \widehat{\beta}_n \right)^2
		- 2 \widehat{\beta}_n x_i (x_i \beta_0 + \epsilon_i) \\
	&= \epsilon_i^2 
		+ x_i^2 \left[ 
					 \beta_0^2 + 
					 \left( \widehat{\beta}_n \right)^2
					 - 2 \left( \beta_0 \widehat{\beta}_n \right)
				\right]
		- 2 x_i \epsilon_i \left(
							\beta_0 \widehat{\beta}_n
							\right), 
	\end{align*}
	which leads us to 
	\begin{align*}
	&\dn \sumin x_i^2 e_i^2 \\
	&= \dn \sumin x_i^2 \epsilon_i^2
		+ \left[ 
				\beta_0^2 + 
				\left( \widehat{\beta}_n \right)^2
				- 2 \left( \beta_0 \widehat{\beta}_n \right)
		  \right] 
		  \left( \dn \sumin x_i^4 \right)
		- 2 \left( \beta_0 \widehat{\beta}_n \right)
			\left( \dn \sumin x_i^3 \epsilon_i \right).  
	\end{align*}
	Continuous Mapping Theorem ensures that 
	$$
	\beta_0^2 
	+ \left( \widehat{\beta}_n \right)^2
	- 2 \left( \beta_0 \widehat{\beta}_n \right) 
	\CONV{a.s.} 0,
	$$
	and 
	$$
	\beta_0 \widehat{\beta}_n
	\CONV{a.s.} \beta_0^2.
	$$
	By assumption \eqref{cond_max_X_i}, we have 
	$$
	\dn \sumin x_i^4 = \mathcal{O} (1).
	$$
	By assumption \eqref{cond_max_X_i} and the Strong Law of Large Numbers, we have
	$$
	\dn \sumin x_i^3 \epsilon_i \CONV{a.s.} 0
	\qquad \text{and} \qquad
	\dn \sumin x_i^2 (\epsilon_i - \sigma^2) \CONV{a.s.} 0.
	$$
	Meanwhile, by assumption \eqref{cond_X'X}, we have
	$$
	\dn \sumin x_i^2 \to c \quad \text{for some} \,\, c > 0.
	$$
	Therefore, by Continuous Mapping Theorem, 
	$$
	\dn \sumin x_i^2 \epsilon_i^2 \CONV{a.s.} \sigma^2 c.
	$$
	Finally, piecing the terms together, we have
	\begin{align*}
	\dn \sumin x_i^2 e_i^2 \CONV{a.s.} \sigma^2 c,
	\end{align*}
	and hence
	$$
	\left( \dn \sumin x_i^2 e_i^2 \right)
	\bigg| \text{data} 
	\CONV{c.p.} \sigma^2 c.
	$$
	A sketch of proof is also provided for the multivariate case. 
	\begin{align*}
	&\dn \sumin e_i^2 \x \x' \\
	&= \dn \sumin \epsilon_i^2 \x \x' \\
	&+ \dfrac{2}{n} \sumin 
					\epsilon_i \x' 
					\left( \bhat - \be_0 \right)
					\x \x' \\
	&+ \dn \sumin
			\left\{
				\x'\left[
						\be_0 \be_0'
						- 2 \bhat \be_0'
						+ \bhat \left( \bhat \right)' 
				\right] \x 
			\right\} \x \x'.
	\end{align*} 
	With our assumptions, the Strong Law of Large Numbers ensures that the first term converges to $\sigma^2 C$ with probability 1 whereas the other two terms converges to zero matrix almost surely. Therefore,
	 $$
	 \left( \dn \sumin e_i^2 \x \x' \right)
	 \bigg| \text{data} 
	 \CONV{c.p.} \sigma^2 C.
	 $$
\end{proof}

\begin{lem} \label{lem_X'Dne_normal}
Assume \eqref{cond_max_X_i}, \eqref{cond_X'X}, and \eqref{cond_epsilon4}. Let $\bLS$ be the OLS estimator for $\be$ in the linear model \eqref{linear.reg.2} with assumptions \eqref{cond_max_X_i} and \eqref{cond_X'X}. Then,
	$$
	\left( 
		\dqn X' D_n \eLS 
	\right) \bigg| \text{data}
	\CONV{c.d.} 
	N \left( \bm{0}, \sigma^2 C \right).
	$$
\end{lem} 

\begin{proof}
	First, note that $\bLS$ is a strongly consistent estimator under assumption \eqref{cond_X'X} \citep*{LSEstrong}. Next,
	\begin{align*}
	&\dqn X' D_n \eLS \\
	&= \left( \dn \sumin e_i^2 \x \x' \right)^{1/2}
		\times \left( \sumin e_i^2 \x \x' \right)^{-1/2}
		\times \sumin e_i \x W_i \\ 
	&= \left( \dn \sumin e_i^2 \x \x' \right)^{1/2}
		\times \left( \sumin e_i^2 \x \x' \right)^{-1/2}
		\times \sumin e_i \x (W_i - 1), 
	\end{align*}
	where the last equality follows from the fact that
	\begin{align*}
	\sumin e_i \x
	&= X' \eLS \\
	&= X'Y - X'X (X'X)^{-1} X'Y \\
	&= \bm{0}.
	\end{align*}
	By Lemma \ref{lem_ave_ei_xi^2}, 
	$$
	\left( \dn \sumin e_i^2 \x \x' \right)^{1/2}
	\bigg| \text{data}
	\CONV{c.p.} 
	\sigma C^{1/2}.
	$$
	Without loss of generality, we will continue our proof for the univariate case. We shall show that the Lindeberg's Central Limit Theorem gives 
	$$
	\left(
		\dfrac{ \sumin e_i x_i (W_i - 1) }
			  { \sqrt{\sumin e_i^2 x_i^2} }
	\right) \bigg| \text{data}
	\CONV{c.d.} N(0, 1)
	$$
	by verifying the following Liapounov's sufficient condition
	$$
	\dfrac{ \sumin \EX 
					\left[ 
						e_i^4 x_i^4 
						(W_i - 1)^4 
						| \text{data}
					\right] 
		  }
		  { \left( Var 
		  			\left[
			  			\sumin e_i x_i (W_i - 1)
			  			| \text{data}
			  		\right] 
			\right)^2 
		  }
	\to 0 
	\quad \text{as} \,\,
	n \to \infty.
	$$
	With assumptions \eqref{cond_max_X_i} and \eqref{cond_epsilon4}, we can use similar reasoning in Lemma \ref{lem_ave_ei_xi^2} to show that 
	$$
	\sumin e_i^4 x_i^4 = \mathcal{O} (n) \,\, \text{a.s.},
	$$
	since $\sumin e_i^4 x_i^4$ can be expanded into
	$$
	\sumin \left[
				x_i^4 \epsilon_i^4 
				- 4 x_i^5 (\widehat{\beta}_n - \beta_0) \epsilon_i^3
				+ 6 x_i^6 (\widehat{\beta}_n - \beta_0)^2 \epsilon_i^2
				- 4 x_i^7 (\widehat{\beta}_n - \beta_0)^3 \epsilon_i
				+ x_i^8 (\widehat{\beta}_n - \beta_0)^4
			\right]. 
	$$ 
	Since $\EX \left[ (W_i - 1)^4 \right] = 9$, 
	$$
	\sumin \EX 
				\left[ 
					e_i^4 x_i^4 
					(W_i - 1)^4 
					| \text{data}
				\right]
			= \mathcal{O} (n)
	$$
	On the other hand, by Lemma \ref{lem_ave_ei_xi^2} and Continuous Mapping Theorem, 
	$$
	\left( \dn \sumin x_i^2 e_i^2 \right)^2
	\CONV{a.s.} 
	\sigma^4 c^2,
	$$
	which implies that
	$$
	\left( \sumin x_i^2 e_i^2 \right)^2 
	= \mathcal{O} (n^2)
	\,\, \text{a.s.}
	$$
	Hence, 
	\begin{align*}
	&\left( Var 
		\left[
			\sumin e_i x_i (W_i - 1)
			\bigg| \text{data}
		\right] 
	\right)^2 \\
	&= \left( 
			\sumin x_i^2 e_i^2 
		\right)^2 
		\bigg| \text{data} \\
	&= \mathcal{O} (n^2)
	\end{align*}
	Therefore, conditional on data, 
	$$
	\sumin e_i^4 x_i^4 \EX \left[ (W_i - 1)^4 \right]
	= o \left[ \left(
				\sumin e_i^2 x_i^2 
				\right)^2 
		\right],
	$$ 
	thus satisfying the Liapounov's sufficient condition. Finally, we apply Lemma \ref{lem_slutsky} to obtain
	$$
	\left( 
		\dqn X' D_n \eLS 
	\right) \bigg| \text{data}
	\CONV{c.d.} 
	N \left( \bm{0}, \sigma^2 C \right).
	$$
\end{proof}

\begin{lem} \label{lem_X'Dne_normal2}
	Assume \eqref{cond_max_X_i}, \eqref{cond_X'X}, and \eqref{cond_epsilon4}. 
	\begin{itemize}
		\item [ (a) ] Let $\bhat \neq \bLS$ be any other strongly consistent estimator for $\be$ in the linear model \eqref{linear.reg.2} under the aforementioned assumptions, and satisfies \eqref{LAS.OLS.shrink.rate}. Then, 
			$$
			\left( 
				\dqn X' D_n \bm{e}_n 
			\right) \bigg| \text{data}
			\CONV{c.d.} 
			N \left( \bm{0}, \sigma^2 C \right),
			$$
		where $\bm{e}_n = Y - X \bhat$. \\
		\item [ (b) ] A Lasso estimator $\bLAS$ with a penalty term $\lambda_n^* = o( \sqrt{n} )$ satisfies \eqref{LAS.OLS.shrink.rate}, and hence can be a substitute for $\bLS$ in parts (a) and (b) of Theorem \ref{asympdistn}. 
	\end{itemize}	
\end{lem}

\begin{proof}
	First, note that
		$$
		\left( 
			\dqn X' (D_n - I) \bm{e}_n 
		\right) \bigg| \text{data}
		\CONV{c.d.} 
		N \left( \bm{0}, \sigma^2 C \right).
		$$
	by Lemmas \ref{lem_ave_ei_xi^2} and \ref{lem_X'Dne_normal}. Now
		$$
		\dqn X' D_n \bm{e}_n = \dqn X' (D_n - I) \bm{e}_n + \dqn X' \bm{e}_n,
		$$
	where, by conditioning on data,
	\begin{align*}
		\dqn X' \bm{e}_n
		&= \dqn X' \eLS   
			+ \dqn X' 
			\left[
				\bm{e}_n - \eLS 		
			\right] \\
		&= \dqn X'X
			\left[
				\bLS - \bhat
			\right] \\
		&= \dn X'X
			\times
			\sqrt{n}
			\left[
				\bLS - \bhat
			\right] \\
		&\CONV{c.p.} \bm{0}, 
	\end{align*}
	by our assumption in part (a) of the Lemma. Finally, by Lemma \ref{lem_slutsky},
		$$
		\left( 
			\dqn X' D_n \bm{e}_n 
		\right) \bigg| \text{data}
		\CONV{c.d.} 
		N \left( \bm{0}, \sigma^2 C \right).
		$$
	For part (b) of the Lemma, first define
		$$
		Q_n^{\text{LAS}} (\bm{z}) 
		:= \left\| 
				(\bm{y} - X \bm{z})
			\right\|_2^2 
			+ \lambda_n^* \| \bm{z} \|_1,
		$$
	which leads to 
	\begin{align*}
	Q_n^{\text{LAS}} \left( \bLS + \dqn \bu \right)
	&= \left\| 
			Y - X \left( \bLS + \dqn \bu \right)				
		\right\|_2^2
		+ \lambda_n^* 
			\left\|
				\bLS + \dqn \bu
			\right\|_1 \\
	&= \left\| 
			\eLS - \dqn X \bu 
		\right\|_2^2
		+ \lambda_n^* 
			\left\|
					\bLS + \dqn \bu
			\right\|_1, 
	\end{align*}
	and 
	\begin{align*}
	Q_n^{\text{LAS}} \left( \bLS  \right)
	&= \left\| 
			Y - X \bLS 
		\right\|_2^2
		+ \lambda_n^* \left\| \bLS \right\|_1 \\
	&= \left\| \eLS	\right\|_2^2
		+ \lambda_n^* \left\| \bLS \right\|_1 .\\
	\end{align*}
	Now define
	$$
	h_n(\bu) := Q_n^{\text{LAS}} \left( \bLS  + \dqn \bu \right) 
				- Q_n^{\text{LAS}} \left( \bLS  \right), 
	$$
	such that 
	$$
	\argmin_{\bu} h_n(\bu)
	= \argmin_{\bu}  Q_n^{\text{LAS}} 
					\left( \bLS  + \dqn \bu \right)
	= \sqrt{n} \left( \bLAS - \bLS \right).
	$$
	Notice that $h_n(\bu)$ can be simplified into 
	$$
	\bu' \left( \dfrac{X' X}{n} \right) \bu
	+ \dfrac{\lambda_n^*}{\sqrt{n}} 
		\left\{
			\left\| \sqrt{n} \bLS + \bu \right\|_1
			- \left\| \sqrt{n} \bLS \right\|_1 
		\right\},
	$$ 
	and $h_n(\bu) \to h(\bu) \equiv \bu' C \bu$ if $\lambda_n^* = o(\sqrt{n})$. Therefore, by the Convexity Lemma \citep{Pollard1991}, and the fact that $\bLAS = O_p(1)$,
	$$
	\sqrt{n} \left( \bLAS - \bLS \right)
	\big| \text{data}
	\CONV{c.p.} \argmin_{\bu} h(\bu) = \bm{0}.
	$$ 
\end{proof}

Now we are ready to prove Theorem \ref{asympdistn}. 

\begin{proof} [Proof of Theorem \ref{asympdistn}]
	Define 
	$$
	Q_n (\bm{z}) := \left\| 
						 	D_n^{\frac{1}{2}} (\bm{y} - X \bm{z})
				    \right\|_2^2 
				    + \lambda_n \| \bm{z} \|_1,
	$$
	which leads to 
	\begin{align*}
	Q_n \left( \bLS + \dqn \bu \right)
	&= \left\| 
			  D_n^{\frac{1}{2}} 
			  \left[
			  		Y - X \left( \bLS + \dqn \bu \right)				
			  \right]	
	   \right\|_2^2
	   + \lambda_n \left\|
	   					 \bLS + \dqn \bu
	   			   \right\|_1 \\
	&= \left\| 
			 D_n^{\frac{1}{2}} 
			\left( 
				 \eLS - \dqn X \bu 
			\right)				
		\right\|_2^2
		+ \lambda_n \left\|
						\bLS + \dqn \bu
					\right\|_1, 
	\end{align*}
	and 
	\begin{align*}
	Q_n \left( \bLS  \right)
	&= \left\| 
			 D_n^{\frac{1}{2}} 
			 \left( 
				  Y - X \bLS 
			 \right)				
		\right\|_2^2
		+ \lambda_n \left\| \bLS \right\|_1 \\
	&= \left\| 
			 D_n^{\frac{1}{2}} \eLS		
	   \right\|_2^2
	   + \lambda_n \left\| \bLS \right\|_1 .\\
	\end{align*}
	Now define
	$$
	V_n(\bu) := Q_n \left( \bLS  + \dqn \bu \right) - Q_n \left( \bLS  \right), 
	$$
	such that 
	$$
	\argmin_{\bu} V_n(\bu)
	= \argmin_{\bu}  Q_n \left( \bLS  + \dqn \bu \right)
	= \sqrt{n} \left( \bnw - \bLS \right).
	$$
	Notice that $V_n(\bu)$ can be simplified into 
	$$
	-2 \bu' \left( \dfrac{X' D_n \eLS}{\sqrt{n}} \right)
	+ \bu' \left( \dfrac{X' D_n X}{n} \right) \bu
	+ \lambda_n \left\{
					\left\| \bLS + \dqn \bu \right\|_1
					- \left\| \bLS \right\|_1 
				\right\}.
	$$
	By Lemma \ref{lem_X'DnX}, 
	$$
	\dn X' D_n X \CONV{p} C.
	$$
	By Lemma \ref{lem_X'Dne_normal}, 
	$$
	\dqn X' D_n \eLS 
	\CONV{c.d.}
	N \left( \bm{0}, \sigma^2 C \right).
	$$
	For the penalty term,
	\begin{align*}
	&\lambda_n \left\{
					\left\| \bLS + \dqn \bu \right\|_1
					- \left\| \bLS \right\|_1 
			   \right\} \\
	&= \dfrac{\lambda_n}{\sqrt{n}} 
		\sum_{j=1}^p
		\left\{
				\left| 
						\sqrt{n} \widehat{\beta}_{n,j}^{\text{OLS}} 
						+ \mu_j
				\right|
			  - \left|
			  			\sqrt{n} \widehat{\beta}_{n,j}^{\text{OLS}}
			   	\right|
		\right\} \\
	&= \dfrac{\lambda_n}{\sqrt{n}} \sum_{j=1}^p
		\left\{
			\left| \sqrt{n}
				\left[
					\beta_{o,j}
					+ \left(
							\widehat{\beta}_{n,j}^{\text{OLS}} - \beta_{o,j} 
					  \right)
				\right]  
				+ \mu_j
			\right|
			- \left| \sqrt{n}
					\left[
						\beta_{o,j}
						+ \left(
							 \widehat{\beta}_{n,j}^{\text{OLS}} - \beta_{o,j} 
						  \right)
					\right]  
			  \right|
		\right\} \\
	&:= \dfrac{\lambda_n}{\sqrt{n}}
		 \sum_{j=1}^p
		 p_n(u_j). 
	\end{align*}
	First consider the case when 
	$$
	\dfrac{\lambda_n}{\sqrt{n}} \to \lambda_0 \in [0, \infty).
	$$
	When $\beta_{o,j} \neq 0$, for large $n$, $\widehat{\beta}_{n,j}^{\text{OLS}} - \beta_{o,j} \CONV{a.s.} 0$, and  $\sqrt{n} \beta_{o,j}$ dominates $u_j$. Hence, it is easy to verify that for $\beta_{o,j} \neq 0$, $p_n(u_j)$ becomes 
	$$
	u_j \text{sgn} \left( \beta_{o,j} \right)
	\mathbbm{1}_{ 
				\left\{ 
						\beta_{o,j} \neq 0 
				\right\} 
				}
	$$
	based on the following observations
	\begin{itemize}
		\item when $\beta_{o,j} >0$ and $u_j >0$, then $p_n(u_j) = u_j$; \\
		\item when $\beta_{o,j} >0$ and $u_j <0$, then $p_n(u_j) = u_j$; \\
		\item when $\beta_{o,j} <0$ and $u_j >0$, then $p_n(u_j) = -u_j$; \\
		\item when $\beta_{o,j} <0$ and $u_j <0$, then $p_n(u_j) = -u_j$. \\ 
	\end{itemize}  
	On the other hand,$\beta_{o,j} = 0$, $p_n(u_j)$ is back to   
	$$
	\left| 
		\sqrt{n} \widehat{\beta}_{n,j}^{\text{OLS}} 
		+ \mu_j
	\right|
	- \left|
		\sqrt{n} \widehat{\beta}_{n,j}^{\text{OLS}}
	\right|,
	$$
	where $\sqrt{n} \widehat{\beta}_{n,j}^{\text{OLS}}$ is normally distributed, and there is no $N>0$ such that $n \geq N \implies \widehat{\beta}_{n,j}^{\text{OLS}} = 0$. This causes $p_n(u_j)$ to depend on sample path when $\beta_{o,j} = 0$:
	$$
	\mathbbm{1}_{ 
				\left\{ 
						\beta_{o,j} \neq 0 
				\right\} 
				}
	\left[
		u_j \text{sgn} 
		\left( \widehat{\beta}_{n,j}^{\text{OLS}} \right)
		\mathbbm{1}_{ 
					\left\{ 
							\widehat{\beta}_{n,j}^{\text{OLS}} \neq 0 
					\right\} 
					}
		+ |u_j| \mathbbm{1}_{ 
							\left\{ 
								\widehat{\beta}_{n,j}^{\text{OLS}} = 0 
							\right\} 
							} 
	\right].
	$$
	Hence, $p_n(u_j)$ becomes 
	$$
	u_j \text{sgn} \left( \beta_{o,j} \right)
	\mathbbm{1}_{ 
		\left\{ 
		\beta_{o,j} \neq 0 
		\right\} 
	} + 
	\mathbbm{1}_{ 
				\left\{ 
						\beta_{o,j} \neq 0 
				\right\} 
				}
	\left[
		u_j \text{sgn} 
		\left( \widehat{\beta}_{n,j}^{\text{OLS}} \right)
		\mathbbm{1}_{ 
					\left\{ 
						\widehat{\beta}_{n,j}^{\text{OLS}} \neq 0 
					\right\} 
					}
		+ |u_j| 
		\mathbbm{1}_{ 
					\left\{ 
						\widehat{\beta}_{n,j}^{\text{OLS}} = 0 
					\right\} 
					} 
	\right].
	$$
	Since $p_n(u_j)$ still depends on sample path anyway, we left the expression as
	$$
	\left[
		u_j \, \text{sgn}(\widehat{\beta}_{n,j}) 
		\mathbbm{1}_{ \{ \widehat{\beta}_{n,j} \neq 0 \}}
		+ | u_j | \mathbbm{1}_{ \{ \widehat{\beta}_{n,j} = 0 \}} 
	\right].
	$$  
	Then, by Lemma \ref{lem_slutsky}, conditional on data, 
	$$
	V_n(\bu) \stackrel{d}{\approx}  V_n^*(\bu) \equiv 
	-2 \bu' \Psi 
	+ \bu' C \bu
	+ \lambda_0 
	  \sum_{j=1}^p
	  \left[
	  	u_j \, \text{sgn}(\widehat{\beta}_{n,j}) 
	 	 \mathbbm{1}_{ \{ \widehat{\beta}_{n,j} \neq 0 \}}
	  	+ | u_j | \mathbbm{1}_{ \{ \widehat{\beta}_{n,j} = 0 \}} 
	  \right], 
	$$
	where $\Psi \sim N \left( \bm{0}, \sigma^2 C \right)$. Finally, conditional on data, $V_n(\bu)$ is convex, and $V_n^*(\bu)$ has unique minimum. Therefore, it follows from \citet{Geyer1996} that
	$$
	\sqrt{n} \left( \bnw - \be_0 \right) \bigg| \text{data}
	= \argmin_{\bu} \left\{ V_n(\bu) \big| \text{data} \right\}
		\stackrel{d}{\approx}
		\argmin_{\bu} 
		\left\{
				V_n^*(\bu) 
				\big| \text{data}
		\right\}.
	$$ 
	Notice that we will arrive at the same result for $p_n(u_j)$ even if we substitute $\bLS$ for a strongly consistent Lasso estimator $\bLAS$ since \citet{Knight&Fu} proved that $\sqrt{n} \left( \widehat{\beta}_{n,j}^{\text{LAS}} - \beta_{0,j} \right)$ is normally distributed, so it is not possible to obtain 
	$$
	\left\|
			\bLAS - \be_0
	\right\|  
	= \mathcal{O} (\sqrt{n}),
	$$
	unless additional assumptions about the matrix $C$ are assumed in part (c), so that $\exists N_\omega$ such that when $\beta_{0,j} = 0 $,
	$$
	n \geq N_\omega
	\implies 
	\widehat{\beta}_{n,j}^{\text{LAS}} = 0. 
	$$
	However, the penalty $\lambda_n^*$ required in part (c) has to be
	$$
	\lambda_n^* 
	= \mathcal{O} ( n ^ {\delta_2} )
	\qquad \text{for some} \quad
	\frac{1}{2} < \delta_2 < 1,	
	$$   
	which fails to satisfy \eqref{LAS.OLS.shrink.rate}. Thus, $\dqn X' \bm{e}_n^{\text{LAS}}$ may not necessarily shrink to zero, and becomes dependent on the sample path $\omega$. This contributes to a moving mean of $\Psi_n$. \\
	
	Finally, if $\dfrac{\lambda_n}{\sqrt{n}} \to 0$, then
	\begin{align*}
	\sqrt{n} \left( \bnw - \be_0 \right) \bigg| \text{data}
	&\CONV{c.d.} \argmin_{\bu} V(\bu) \\
	&= \argmin_{\bu} \left\{ 
						- 2 \bu' \Psi + \bu' C \bu 
					\right\}  \\
	&= C^{-1} \Psi \sim N (\bm{0}, \sigma^2 C^{-1}).
	\end{align*}
\end{proof}
   
\begin{lem} \label{lem_model.select}
	Assume all conditions stated in Theorem \ref{model.select.consistent}. Then
	$$
	P\left(
		\bnw (\lambda_n) \stackrel{s}{=} \be_0
		\bigg| \text{data}
	\right)	
	\geq P \left( 
				A_n^w \cap B_n^w 
				\big| \text{data}
			\right), 
	$$ 
	where
	\begin{align*}
	A_n^w &\equiv 
	\left\{
		\left|
			\left( \cnwa \right)^{-1} \znwa 
		\right| 
		< \sqrt{n}
			\left[
				\left| \be_{0(1)} \right|
				- \dfrac{\lambda_n}{2n} 
					\left|
						\left( \cnwa \right)^{-1} 
						\text{sgn} 
						\left( \be_{0(1)} \right) 
					\right|
			\right]
		\, \text{element-wise}
	\right\} \\
	B_n^w &\equiv
	\left\{
		\left|
			\cnwc \left( \cnwa \right)^{-1} \znwa - \znwb 
		\right| 
		\leq \dfrac{\lambda_n}{2 \sqrt{n}}
			\left(
				\bm{\eta} 
				- \left| \bm{\rho}_n^w \right| 
			\right)
		\, \text{element-wise}
	\right\},
	\end{align*}
	for
	\begin{align*}
	\znwa &= \dqn X_{n(1)}' D_n \ep_n, \\
	\znwb &= \dqn X_{n(2)}' D_n \ep_n, \\
	\bm{\rho}_n^w  
	&= \left[
			\cnwc \left(\cnwa\right)^{-1}
			- C_{n(21)} \left( C_{n(11)} \right)^{-1} 
		\right]
		\text{sgn} 
		\left( \be_{0(1)} \right).
	\end{align*}
\end{lem}

\begin{proof}
	Based on
	$$
	Q_n(\bm{z}) 
	= \left\| 
			D_n^{\frac{1}{2}} 
			\left( Y - X \bm{z} \right)
	  \right\|_2^2
	  + \lambda_n
	  	\left\| \bm{z} \right\|_1, 
	$$
	we consider $Q_n(\be_0 + \bu_n)$ and drop the terms that do not involve $\bu_n$ to obtain
	$$
	V_n(\bu_n) 
	= - 2 \bu_n' \left(X' D_n \ep_n\right)
	+ \bu_n' \left( X' D_n X \right) \bu 
	+ \lambda_n 
		\left\{  
			\left\| \be_0 + \bu_n \right\|_1 
		\right\},
	$$
	such that
	\begin{align*}
	\left(
		\bnw - \be_0 
	\right) 
	\bigg| \text{data} 
	&= \argmin_{\bu_n}
		\left\{
			Q_n(\be_0 + \bu_n)
			\big| \text{data}
		\right\} \\
	&= \argmin_{\bu_n}
		\left\{
			V_n(\bu_n)
			\big| \text{data}
		\right\}.
	\end{align*}
	Differentiating the first two terms with respect to $\bu_n$ yields
	\begin{align*}
		&2 X' D_n X \bu_n - 2 X' D_n \ep_n \\
		&= 2 \sqrt{n} 
			\left( \dn X' D_n X \right)
			\left( \sqrt{n} \bu_n \right) 
		  - 2 \sqrt{n} 
		  	\left( \dqn X' D_n \ep_n \right) \\
		&= 2 \sqrt{n} 
			\left[
				C_n^w 
				\left( \sqrt{n} \bu_n \right) 
				- \bm{Z}_n^w
			\right] 
	\end{align*}
	Note that $\bnw = \widehat{\bu}_n + \be_0$, which can be partitioned into 
	\[
		\bnw = 
		\begin{bmatrix}
			\widehat{\be}^w_{n(1*)} \\
			\widehat{\be}^w_{n(2*)} 
		\end{bmatrix},
	\]
	where $\widehat{\be}^w_{n(1*)}$ consists of non-zero elements of $\bnw$ and $\widehat{\be}^w_{n(2*)} = \bm{0}$. The asterisk here is to distinguish the partition of $\bnw$ from the partition of $\be_0$. If both partitions are the same, then the Weighted Lasso Bootstrap selects the true model. Based on the partition of $\bnw$, we have      
	\begin{align*}
		&2 \sqrt{n} 
			\left[
				C_n^w 
				\left( \sqrt{n} \widehat{\bu}_n \right) 
				- \bm{Z}_n^w
			\right] \\
		&= 2 \sqrt{n} 
			\left\{
				\begin{bmatrix}
					\cnwas & \cnwbs \\
					\cnwcs & \cnwds 
				\end{bmatrix}
				\times \sqrt{n} 
				\begin{bmatrix}
					\hunas \\
					\hunbs
				\end{bmatrix} 
				-
				\begin{bmatrix}
					\znwas \\
					\znwbs
				\end{bmatrix}
			\right\}.
	\end{align*}
	As a consequence of the Karush-Kuhn-Tucker (KKT) conditions, we have 
	\begin{align}
		&2 \sqrt{n} 
		\left\{
			\sqrt{n} 
			\left[
				\cnwas \hunas + \cnwbs \hunbs 
			\right]
			- \znwas
		\right\}
		= - \lambda_n \text{sgn} 
			\left( \widehat{\be}^w_{n(1*)} \right)\notag \\
		&\implies \cnwas \left[\sqrt{n} \hunas \right] 
				+ \cnwbs \left[\sqrt{n} \hunbs \right] 
				- \znwas
			= - \dfrac{\lambda_n}{ 2 \sqrt{n} }
				\text{sgn} \left( \widehat{\be}^w_{n(1*)} \right), 
	\end{align}
	and
	\begin{align}
		&\left|
			2 \sqrt{n}
			\left\{
				\sqrt{n} 
				\left[
					\cnwcs \hunas + \cnwds \hunbs
				\right]
				- \znwbs 
			\right\}
		\right|
		\leq \lambda_n \bm{J} \notag \\
		&\implies 
		 \left|
		 	\cnwcs \left[ \sqrt{n} \hunas \right]
		 	+ \cnwds \left[ \sqrt{n} \hunbs \right]
		 	- \znwbs
		 \right|
		 \leq \dfrac{\lambda_n}{2 \sqrt{n}} \bm{J}
	\end{align}
	element wise. Hence, conditional on data, if there exists $\widehat{\bu}_n$ such that the following equality and inequalities hold:
	\begin{align}
	\cnwa \left[ \sqrt{n} \huna \right] - \znwa 
	= - \dfrac{\lambda_n}{2 \sqrt{n}}
		\text{sgn} \left( \widehat{\be}^w_{n(1)} \right) \label{eq.ineq.1} \\
	- \dfrac{\lambda_n}{2 \sqrt{n}} \bm{J} \leq
	\cnwc \left[ \sqrt{n} \huna \right] - \znwb 
	\leq  \dfrac{\lambda_n}{2 \sqrt{n}} \bm{J}
	\quad \text{element wise} \label{eq.ineq.2} \\
	\left| \huna \right| < \left| \be_{0(1)} \right|
	\quad \text{element wise} \label{eq.ineq.3} ,
	\end{align} 
	then we have
	$$
	\text{sgn} \left[ \widehat{\be}^w_{n(1)} \right]
	= \text{sgn} \left[ \be_{0(1)} \right]
	\qquad \text{and} \qquad
	\hunb = \widehat{\be}^w_{n(2)} = \be_{0(2)} = \bm{0},
	$$
	ie. $\bnw \stackrel{s}{=} \be_0$. This also implies that
	\begin{equation*}
		\begin{split}
		P \left( \bnw \stackrel{s}{=} \be_0 \bigg| \text{data} \right) \geq 
		P \bigg( 
				 &\left\{
				 			\left| \cnwc \left[ \sqrt{n} \huna \right] - \znwb \right| 
				 			\leq  \dfrac{\lambda_n}{2 \sqrt{n}} \bm{J}
				 			\quad \text{element wise}
				  \right\} \\
				 &\bigcap \left\{
				 			\cnwa \left[ \sqrt{n} \huna \right] - \znwa 
				 			= - \dfrac{\lambda_n}{2 \sqrt{n}}
				 			\text{sgn} \left( \widehat{\be}^w_{n(1)} \right)
				 			\right\} \\
				 &\bigcap \left\{
				 				\left| \huna \right| < \left| \be_{0(1)} \right|
				 				\quad \text{element wise}
				 			\right\}
				 \bigg| \text{data}
		  \bigg).
		\end{split}
	\end{equation*}
	We now simplify the intersection of events on the RHS of the inequality. From \eqref{eq.ineq.1}, 
	\begin{align*}
		\left( \cnwa \right)^{-1} \znwa 
		&= \sqrt{n} \huna
		+ \dfrac{\lambda_n}{2 \sqrt{n}} 
			\left( \cnwa \right)^{-1} 
			\text{sgn} \left(\be_{0(1)} \right)	\\
		\implies 
			\left| \left( \cnwa \right)^{-1} \znwa \right|
			&\leq \sqrt{n} \left| \huna \right|
			+ \dfrac{\lambda_n}{2 \sqrt{n}} 
			 \left| 
			 	\left( \cnwa \right)^{-1} 
				\text{sgn} \left(\be_{0(1)} \right)
			 \right|.
	\end{align*}
	Substitute \eqref{eq.ineq.3} into the inequality above yields
	$$
	\left| \left( \cnwa \right)^{-1} \znwa \right|
	\leq \sqrt{n} 
		 \left\{
		 		\left| \be_{0(1)} \right|
		 		+ \dfrac{\lambda_n}{2 n} 
		 			\left| 
		 				\left( \cnwa \right)^{-1} 
		 				\text{sgn} \left(\be_{0(1)} \right)
		 			\right|
		 \right\},
	$$
	which corresponds to $A_n^w$. Also, from \eqref{eq.ineq.1},
	$$
	\sqrt{n} \huna 
	= \left( \cnwa \right)^{-1}  \znwa 
	- \dfrac{\lambda_n}{2 \sqrt{n}}
		\left( \cnwa \right)^{-1} 
		\text{sgn} \left( \be_{0(1)} \right).
	$$
	Substitute this equality into \eqref{eq.ineq.2} yields
	$$
	\left|
		\cnwc \left( \cnwa \right)^{-1} \znwa - \znwb 
		- \dfrac{\lambda_n}{2 \sqrt{n}}
		  \cnwc \left( \cnwa \right)^{-1}
		  \text{sgn} \left( \be_{0(1)} \right)
	\right|
	\leq \dfrac{\lambda_n}{2 \sqrt{n}} \bm{J} 
	$$
	element wise, which can be expanded into
	$$
	\left|
		\cnwc \left( \cnwa \right)^{-1} \znwa - \znwb 
		- \dfrac{\lambda_n}{2 \sqrt{n}}
		  \left[
		  		C_{n(21)} \left( C_{n(11)} \right)^{-1}
		  		\text{sgn} \left( \be_{0(1)} \right)
		  		+ \bm{\rho}_n^w
		  \right]
	\right|
	\leq \dfrac{\lambda_n}{2 \sqrt{n}} \bm{J} 
	$$
	element wise. Now, from the Irrepresentable assumption,
	\begin{align*}
		\left|
			C_{n(21)} \left( C_{n(11)} \right)^{-1} 
			\text{sgn} \left( \be_{0(1)} \right)
		\right| &\leq
		\bm{J} - \bm{\eta} \\
		\implies 
		\left|
			C_{n(21)} \left( C_{n(11)} \right)^{-1} 
			\text{sgn} \left( \be_{0(1)} \right)
			+ \bm{\rho}_n^w
		\right|
		&\leq \bm{J} 
		- \left(
		  		\bm{\eta}  - \left| \bm{\rho}_n^w \right|
		  \right).
	\end{align*} 
	element wise. Consequently, we must have
	$$
	\left|
		\cnwc \left( \cnwa \right)^{-1} \znwa - \znwb 
	\right| 
	\leq \dfrac{\lambda_n}{2 \sqrt{n}}
	\left(
		\bm{\eta} 
		- \left| \bm{\rho}_n^w \right| 
	\right)
	$$
	element wise, which corresponds to $B_n^w$. Therefore, 
	$$
	P\left(
			\bnw (\lambda_n) \stackrel{s}{=} \be_0
			\bigg| \text{data}
	  \right)	
	\geq P \left( 
				A_n^w \cap B_n^w 
				\big| \text{data}
			\right).
	$$ 
\end{proof}

\begin{lem} \label{lem_Anwc}
	Assume all conditions stated in Theorem \ref{model.select.consistent}. Then
	$$
	P \left[	
			\left( A_n^w  \right)^c
			\big| \text{data}
	\right]
	= o \left( e^{-n} \right).
	$$
\end{lem}

\begin{proof}
	From Lemma \ref{lem_X'DnX}, we have 
	$$
	\dn X_{(1)}' D_n X_{(1)} = \cnwa \CONV{a.s.} C_{11}.
	$$
	From Lemmas \ref{lem_ave_ei_xi^2} and \ref{lem_X'Dne_normal}, we have
	\begin{align*}
		\dqn X_{(1)}' (D_n - I) \ep_n 
		\bigg| \text{data} 
		&\CONV{c.d.} 
		N \left( \bm{0}, \sigma^2 C_{11} \right) \\
		\implies \dqn X_{(1)}' D_n \ep_n
		\bigg| \text{data} 
		= \znwa \big| \text{data}
		&\stackrel{d}{\approx}
		N \left( \dqn X_{(1)}' \ep_n , \sigma^2 C_{11} \right) \\
		\implies \left( \cnwa \right)^{-1} \znwa 
		\bigg| \text{data}
		&\stackrel{d}{\approx}
		N \left[ 
				\dqn C_{11}^{-1} X_{(1)}' \ep_n 
				, \sigma^2 C_{11}^{-1} 
		  \right].
	\end{align*}
	For brevity, we introduce the following notations:
	\begin{align*}
		\bm{\kappa}_n = \left[ \kappa_{n,1} \ldots \kappa_{n,q} \right]'
		&\equiv \left( \cnwa \right)^{-1} \znwa \\
		\bm{\mu}_n = \left[ \mu_{n,1} \ldots \mu_{n,q} \right]' 
		&\equiv \dqn \left( C_{11} \right)^{-1} X_{(1)}' \ep_n \\
		\bm{b}_n = \left[ b_{n,1} \ldots b_{n,q} \right]'
		&\equiv \left( \cnwa \right)^{-1} 
				\text{sgn} \left( \be_{0(1)} \right).
	\end{align*}
	It is easy to note that $\bm{b}_n = \mathcal{O}(1)$ since
	$$
	\bm{b}_n \to C_{11}^{-1} \text{sgn} \left( \be_{0(1)} \right). 
	$$
	In addition, by Lemma 3.1 of \citet{Chatterjee&Lahiri}, 
	\begin{align*}
		&\dqn \bm{\mu}_n
		= \left( C_{11} \right)^{-1} \dn X_{(1)}' \ep_n 
		\CONV{a.s.} \bm{0} \\
		&\implies \dqn \bm{\mu}_n \bigg| \text{data}
		\CONV{c.p.} \bm{0}.
	\end{align*}
	Furthermore, from above, it is clear that each of $\kappa_{n,j}$ for $j = 1, \ldots, q$ has a normal limiting distribution with finite variance $\sigma^2_{\kappa,j}$. Then there exists $\sigma^2_{\kappa}$ such that 
	$$
	\sigma^2_{\kappa,j} \leq \sigma^2_{\kappa} 
	\quad \forall \quad j = 1, \ldots, q, 
	$$ 
	and let 
	$$
	\tau_{n,j} = \dfrac{ \kappa_{n,j} - \mu_{n,j} }{ \sigma_{\kappa,j} },
	$$
	which has a standard Normal limiting distribution. Since
	$$
	\left(A_n^w \right)^c = 
	\left\{
		\left|
			\left( \cnwa \right)^{-1} \znwa 
		\right| 
	\geq \sqrt{n}
	  \left[
			\left| \be_{0(1)} \right|
			- \dfrac{\lambda_n}{2n} 
			\left|
				\left( \cnwa \right)^{-1} 
				\text{sgn} 
				\left( \be_{0(1)} \right) 
			\right|
  	  \right]
	  \, \text{element-wise}
	\right\},
	$$ 
	then it follows that
	\begin{align*}
		P \left[
		  		\left( A_n^w \right)^c
		  		\bigg| \text{data}
		  \right]
		&= P \left[
			 		\bigcap^q_{j=1} 
			 		\left\{
			 			| \kappa_{n,j} | \geq \sqrt{n} 
			 			\left(
			 				|\beta_{0,j}| + \dfrac{\lambda_n}{2n} b_{n,j}
			 			\right)
			 		\right\}
			 		\bigg| \text{data}
			 \right] \\
		&\leq \sum_{j=1}^q 
		 P \left[
		 		| \kappa_{n,j} | \geq \sqrt{n}
		 		\left(
		 			|\beta_{0,j}| + \dfrac{\lambda_n}{2n} b_{n,j}
		 		\right)
		 		\bigg| \text{data} 
		   \right] \\
		&= \sum_{j=1}^q
			P \left(
					\tau_{n,j} \geq \dfrac{ \sqrt{n} }{ \sigma_{\kappa,j} }
					\left[
						|\beta_{0,j}|  + \dfrac{\lambda_n}{2n} b_{n,j} - \dqn \mu_{n,j}
					\right] 
					\bigg| \text{data}
			   \right) \\
	 	&\quad + \sum_{j=1}^q
	 		P \left(
	 				\tau_{n,j} \leq - \dfrac{ \sqrt{n} }{ \sigma_{\kappa,j} }
	 				\left[
	 					|\beta_{0,j}|  + \dfrac{\lambda_n}{2n} b_{n,j} + \dqn \mu_{n,j}
	 				\right]
	 				\bigg| \text{data}
	 		  \right) \\ 
	 	&\leq 2 \sum_{j=1}^q
	 	 \left[
	 	 	1 - \Phi \left\{
	 	 					\dfrac{ |\beta_{0,j}| + o(1) }{ \sigma_{\kappa} / \sqrt{n} }
	 	 			\right\}
	 	 \right] \\
	 	&\leq 2 \sum_{j=1}^q
	 	 \dfrac{ \sigma_{\kappa} }
	 	 	   { \left[ 1 + o(1) \right] \sqrt{n} |\beta_{0,j}| }  
	 	 \exp \left\{
	 	 	  		- \dfrac{ n \left[ \beta_{0,j} + o(1) \right]^2 }
	 	 	  				  { 2 \sigma_{\kappa}^2 }
	 	 	  \right\} \\
	 	&= o \left( e ^ {-n} \right),
	\end{align*}
	where the second last line follows from the well-known Gaussian tail bound
	\begin{align} \label{Gaussian.tail.bound}
	1 - \Phi(t) \leq t^{-1} e^ {- t^2}.
	\end{align}
\end{proof}

\begin{lem} \label{lem_CW*_DN_ep_norm}
	Assume all conditions stated in Theorem \ref{model.select.consistent}. Denote
	$$
	\left( C_n^{w*} \right)'
	\equiv 
	\left[
		\left(
			\dn X_{(2)}' D_n X_{(2)}
		\right)
		\left(
			\dn X_{(1)}' D_n X_{(1)}
		\right)^{-1}
		X_{(1)}' - X_{(2)}'
	\right].
	$$
	Then
	$$
	\dqn \left( C_n^{w*} \right)' (D_n - I) \ep_n 
	\bigg| \text{data}
	\CONV{c.d.} 
	N \left( \bm{0}, \Sigma \right),
	$$
	where $\Sigma = \sigma^2 \left( C_{22} - C_{21} C_{11}^{-1} C_{21}' \right)$.
\end{lem}

\begin{proof}
	Let $\cnwsi$ be the $i$-th row of the matrix $C_n^{w*}$, so that
	\begin{align*}
		&\dqn \left( C_n^{w*} \right)' (D_n - I) \ep_n \\ 
		&= \left[
	  			\dn \sumin \epsilon_i^2 \cnwsi \cnwsi' 
	  	  \right]^{\frac{1}{2}}
	  	  \times
	  	  \left[
	  			\dn \sumin \epsilon_i^2 \cnwsi \cnwsi'
	  	  \right]^{-\frac{1}{2}}
	   	  \sumin \cnwsi \epsilon_i (W_i - 1).
	\end{align*}
	First, we want to show that 
	$$
	\dn \sumin \epsilon_i^2 \cnwsi \cnwsi'
	\bigg| \text{data}
	\CONV{c.p.} \Sigma.
	$$
	We begin with 
	\begin{align*}
		&\dn \sumin \cnwsi \cnwsi' \\
		&= \dn \left( C_n^{w*} \right)' \left( C_n^{w*} \right) \\ 
		&= \dn
			\left[
				\cnwc \left( \cnwa \right)^{-1} X_{(1)}' - X_{(2)}'
			\right] 
			\left[
				X_{(1)} \left( \cnwa \right)^{-1} \left( \cnwc \right)'  - X_{(2)}
			\right] \\
		&\CONV{a.s.} C_{21} C_{11}^{-1} C_{11} C_{11}^{-1} C_{21}'
					+ C_{22}  - 2 C_{21} C_{11}^{-1} C_{21}' \\
		&= C_{22} - C_{21} C_{11}^{-1} C_{21}'. 
	\end{align*}
	By Lemma \ref{lem_X'DnX} and assumption \eqref{cond_max_X_i},
	$$
	\underset{1 \leq i \leq n}{max} \left\| \bm{c}_{n,i}^{w*} \right\|^2_2
	= \mathcal{O}(1) \quad \text{a.s.}
	$$
	Then, by Strong Law of Large Numbers,
	$$
	\dn \sumin \left( \epsilon_i^2 - \sigma^2 \right) \cnwsi \cnwsi' 
	\CONV{a.s.} \bm{0}.
	$$
	Hence, by Continuous Mapping Theorem,
	\begin{align*}
		&\dn \sumin \epsilon_i^2 \cnwsi \cnwsi'
			\CONV{a.s.} \sigma^2 
			\left[ 
				C_{22} - C_{21} C_{11}^{-1} C_{21}' 
			\right] 
			= \Sigma \\
		&\implies \dn \sumin \epsilon_i^2 \cnwsi \cnwsi'
			\bigg| \text{data} \CONV{c.p.} \Sigma.  
	\end{align*}
	For the second part of the proof, we want to show that Lindeberg's Central Limit Theorem gives 
	$$
	\left[
		\dn \sumin \epsilon_i^2 \cnwsi \cnwsi'
	\right]^{-\frac{1}{2}}
	\left[
		\sumin \cnwsi \epsilon_i (W_i - 1)
	\right]
	\bigg| \text{data}
	\CONV{c.d.}
	N \left(\bm{0}, I \right).
	$$
	Without loss of generality, we show that the Liapounov's sufficient condition is satisfied for the univariate case:
	$$
	\sumin 9 \epsilon_i^4 \left( c_{n,i}^{w*} \right)^4
	= o \left(
			\left[
					\sumin \epsilon_i^2 \left( c_{n,i}^{w*} \right)^2 
			\right]^2
		\right)
	\quad \text{given data}.
	$$
	By Lemma \ref{lem_X'DnX} and assumptions \eqref{cond_max_X_i} and \eqref{cond_epsilon4}, 
	$$
	\sumin 9 \epsilon_i^4 \left( c_{n,i}^{w*} \right)^4
	= \mathcal{O}(n) \quad \text{given data}.
	$$ 
	Based on the first part of the proof, we can see that
	$$
	\left[
		\sumin \epsilon_i^2 \left( c_{n,i}^{w*} \right)^2 
	\right]^2 
	= \mathcal{O}(n^2) \quad \text{given data}. 
	$$
	Thus Liapounov's sufficient condition is satisfied. Finally, by Lemma \ref{lem_slutsky},
	$$
	\dqn \left( C_n^{w*} \right)' (D_n - I) \ep_n 
	\bigg| \text{data}
	\CONV{c.d.} 
	N \left( \bm{0}, \Sigma \right).
	$$
\end{proof}

\begin{lem} \label{lem_Bnwc}
	Assume all conditions stated in Theorem \ref{model.select.consistent}. Then
	$$
	P \left[	
	\left( B_n^w  \right)^c
	\big| \text{data}
	\right]
	= o \left( e^{ -n^ { 2c - 1 } } \right).
	$$
\end{lem}

\begin{proof}
	First note that
	$$
	\left( B_n^w \right)^c =
	\left\{
			\left|
				\cnwc \left( \cnwa \right)^{-1} \znwa - \znwb 
			\right| 
			> \dfrac{\lambda_n}{2 \sqrt{n}}
			 \left(
				\bm{\eta} 
				- \left| \bm{\rho}_n^w \right| 
			 \right)
			\, \text{element-wise}
	\right\}.
	$$
	By Lemma \ref{lem_X'DnX} and assumption \ref{cond_X'X},
	\begin{align*}
	\bm{\rho}_n^w  
	&= \left[
			\cnwc \left(\cnwa\right)^{-1}
			- C_{n(21)} \left( C_{n(11)} \right)^{-1} 
	  \right]
	  \text{sgn} 
	  \left( \be_{0(1)} \right) \\
	 &\CONV{a.s.} 
	 \left[
	 		C_{21} \left( C_{11} \right)^{-1}
	 	  - C_{21} \left( C_{11} \right)^{-1} 
	 \right]
	 \text{sgn} 
	 \left( \be_{0(1)} \right)\\
	 &= \bm{0},
	\end{align*}
	and hence, $\bm{\rho}_n^w = o(1)$ a.s. Next, using the notations from Lemma \ref{lem_CW*_DN_ep_norm},
	\begin{align*}
		&\cnwc \left( \cnwa \right)^{-1} \znwa - \znwb \\
		&= \dqn \left[
					\left(
						\dn X_{(2)}' D_n X_{(2)}
					\right)
					\left(
						\dn X_{(1)}' D_n X_{(1)}
					\right)^{-1}
					X_{(1)}' - X_{(2)}'
				\right]
			D_n \ep_n \\
		&= \dqn \left( C_n^{w*} \right)' D_n \ep_n. 
	\end{align*}
	From Lemma \ref{lem_CW*_DN_ep_norm},
	\begin{align*}
	&\dqn \left( C_n^{w*} \right)' (D_n - I) \ep_n 
	\bigg| \text{data}
	\CONV{c.d.} 
	N \left( \bm{0}, \Sigma \right) \\
	&\implies \dqn \left( C_n^{w*} \right)' D_n \ep_n
	\bigg| \text{data}
	\stackrel{d}{\approx}
	N \left( 
			C_{22} C_{11}^{-1} \dqn X_{(1)}' \ep_n 
			- \dqn X_{(2)}' \ep_n , 
			\Sigma 
	  \right). 
	\end{align*}
	Again, for brevity, we introduce the following notations:
	\begin{align*}
		\bm{\xi}_n = \left[ \xi_{n,1} \ldots \xi_{n,p-q} \right]'
		&\equiv \cnwc \left( \cnwa \right)^{-1} \znwa - \znwb \\
		\bm{\nu}_n = \left[ \nu_{n,1} \ldots \nu_{n,p-q} \right]'
		&\equiv C_{22} C_{11}^{-1} \dqn X_{(1)}' \ep_n - \dqn X_{(2)}' \ep_n. 
	\end{align*}
	By Lemma 3.2 of \citet{Chatterjee&Lahiri}, 
	\begin{align*}
		&\dfrac{1}{ n ^ {c - \frac{1}{2} } } \bm{\nu}_n \\
		&= C_{21} C_{11}^{-1} \dfrac{1}{n^c} X_{(1)}' \ep_n
			-  \dfrac{1}{n^c} X_{(2)}' \ep_n \\
		&\CONV{a.s.} \bm{0}. 
	\end{align*}
	Besides that, note that each of $\xi_{n,j}$ for $j = 1, \ldots, p-q$ has a Normal limiting distribution with finite variance $\sigma^2_{\xi,j}$. Then, there exists $\sigma^2_{\xi}$ such that
	$$
	\sigma^2_{\xi,j} \leq \sigma^2_{\xi}
	\quad \forall \quad j = 1, \ldots, p-q, 
	$$ 
	and let 
	$$
	\zeta_{n,j} = \dfrac{ \xi_{n,j} - \nu_{n,j} }
						{ \sigma_{\xi,j} },
	$$
	which has a standard Normal limiting distribution. Therefore,
	\begin{align*}
		P \left[
		  		\left( B_n^w \right)^c
		  		\big| \text{data}
		  \right]
		&= P \left(
					\bigcap_{j=1}^{p-q} 
					\left\{
							\left| \xi_{n,j} \right| > 
							\dfrac{\lambda_n}{ 2 \sqrt{n} }
							\left( 
								\eta_j - \left| \rho^w_{n,j} \right| 
							\right)
					\right\}
					\bigg| \text{data}
			\right) \\
		&\leq \sum_{j=1}^{p-q} 
				P \left(
				  		\left| \xi_{n,j} \right| >
				  		\dfrac{\lambda_n}{ 2 \sqrt{n} }
				  		\left(
				  			\eta_j - \left| \rho^w_{n,j} \right|
				  		\right)
				  \bigg| \text{data}		
				  \right) \\
		&= \sum_{j=1}^{p-q}
			P \left(
					\zeta_{n,j} >
					\dfrac{ \frac{\lambda_n}{ 2 \sqrt{n} } 
						    \left(
						    		\eta_j - \left| \rho^w_{n,j} \right|
						    \right)
						    - \nu_{n,j}
					      }
						  { \sigma_{\xi,j} } 
			  \Biggm| \text{data}
			  \right) \\
		&\quad + \sum_{j=1}^{p-q}
				P \left(
						\zeta_{n,j} <
						- \dfrac{ \frac{\lambda_n}{ 2 \sqrt{n} } 
								\left(
									\eta_j - \left| \rho^w_{n,j} \right|
								\right)
								+ \nu_{n,j}
								}
								{ \sigma_{\xi,j} } 
						\Biggm| \text{data}
				  \right) \\
		&= \sum_{j=1}^{p-q}
			P \left(
			  		\zeta_{n,j} > \dfrac{ n^{ c - \frac{1}{2} } }
			  							{ 2 \sigma_{\xi,j} }
			  		\left[
			  			 \frac{\lambda_n}{n^c} \eta_j
			  			- \frac{\lambda_n}{n^c} \left| \rho^w_{n,j} \right|
			  			- \frac{ 2 \nu_{n,j}  }{ n^{ c - \frac{1}{2} } } 
			  		\right]
			  \Biggm| \text{data}
			  \right) \\
		&\quad + \sum_{j=1}^{p-q}
			P \left(
					\zeta_{n,j} < - \dfrac{ n^{ c - \frac{1}{2} } }
										  { 2 \sigma_{\xi,j} }
					\left[
						 \frac{\lambda_n}{n^c} \eta_j
						- \frac{\lambda_n}{n^c} \left| \rho^w_{n,j} \right|
						+ \frac{ 2 \nu_{n,j}  }{ n^{ c - \frac{1}{2} } } 
					\right]
				\Biggm| \text{data}
				\right) \\
		&\leq 2 \sum_{j=1}^{p-q}
		  \left[
		 	1 - \Phi \left\{
		 					\dfrac{ n^{ c - \frac{1}{2} } }{ 2 \sigma_{\xi} }
		 					\left[
		 						\frac{\lambda_n}{n^c} \eta_j + o(1)	
		 					\right]
		 			 \right\}
		  \right] \\
		&\leq 2 \sum_{j=1}^{p-q}
		 \dfrac{ 2 \sigma_{\xi} }
		 	   { n^{ c - \frac{1}{2} } 
		 	   	 \left[
		 	   	 		\frac{\lambda_n}{n^c} \eta_j + o(1)
		 	   	 \right]
	 	   	   }
 	   	  \exp \left\{
 	   	  			- \dfrac{1}{2}
 	   	  			\left(
 	   	  				\dfrac{ n^{ c - \frac{1}{2} } }{ 2 \sigma_{\xi} }
 	   	  				\left[
 	   	  					\frac{\lambda_n}{n^c} \eta_j + o(1)	
 	   	  				\right]
 	   	  			\right)^2
 	   	  		\right\} \\
 	   	&= o \left( e ^ {-n ^ {2c-1} } \right),
	\end{align*}
	where the second last line follows from \eqref{Gaussian.tail.bound}.
\end{proof}

We are now ready to prove Theorem \ref{model.select.consistent}.

\begin{proof}[Proof of Theorem \ref{model.select.consistent}]
	From Lemma \ref{lem_model.select},
	\begin{align*}
		P\left(
			\bnw (\lambda_n) \stackrel{s}{=} \be_0
			\bigg| \text{data}
		 \right)	
		 &\geq P \left( 
					A_n^w \bigcap B_n^w 
					\big| \text{data}
				 \right) \\
		 &= 1- P \left[
					\left( 
						A_n^w \bigcap B_n^w 
					\right)^c
					\bigg| \text{data}
				\right] \\
		 &= 1- P \left[
						\left( A_n^w \right)^c 
						\bigcup 
						\left( B_n^w  \right)^c
						\bigg| \text{data}
				 \right] \\
		&\geq 1 - \left\{
						P \left[ 
								\left( A_n^w \right)^c
								\bigg| \text{data} 
						  \right]
					  + P \left[ 
					  			\left( B_n^w \right)^c
					  			\bigg| \text{data} 
					  	  \right]
				  \right\} \\
		&= 1 - o \left( e ^ {-n^ {2c - 1} } \right),
	\end{align*}
	where the last line follows from Lemmas \ref{lem_Anwc} and \ref{lem_Bnwc}.
\end{proof}

\bibliographystyle{asa}     
\bibliography{WLB_Ref}         

\end{document}